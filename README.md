# ğŸ¥ GEMAA - Google MedGemma Local Environment# MedGemma Local Environment



A complete local setup for working with **Google MedGemma 4B** AI model for medical question answering and analysis.This project sets up a Google Colab-like environment for working with MedGemma (medical language models based on Google's Gemma) locally.



---## ğŸš€ Quick Start



## ğŸ“ Project Structure### 1. Install Dependencies

```powershell

```.venv\Scripts\python.exe -m pip install --upgrade pip

GEMAA/.venv\Scripts\python.exe -m pip install -r requirements.txt

â”‚```

â”œâ”€â”€ ğŸ““ notebooks/              # Jupyter notebooks

â”‚   â”œâ”€â”€ medgemma_actual.ipynb       # Main MedGemma 4B notebook (âœ… Token configured)### 2. Launch Jupyter Notebook

â”‚   â”œâ”€â”€ real_world_datasets.ipynb  # Real medical datasets analysis```powershell

â”‚   â”œâ”€â”€ start_here.ipynb            # Sample data analysis.venv\Scripts\jupyter notebook

â”‚   â””â”€â”€ test_cells.ipynb            # Environment testing```

â”‚Or for JupyterLab:

â”œâ”€â”€ ğŸ”§ scripts/                # Python & PowerShell scripts```powershell

â”‚   â”œâ”€â”€ launch_medgemma.ps1         # ğŸš€ One-click MedGemma launcher.venv\Scripts\jupyter lab

â”‚   â”œâ”€â”€ launch_jupyter.ps1          # Launch Jupyter Notebook```

â”‚   â”œâ”€â”€ diagnose_jupyter.py         # Diagnose setup issues

â”‚   â”œâ”€â”€ test_setup.py               # Test environment### 3. Open the Setup Notebook

â”‚   â””â”€â”€ verify_setup.py             # Verify dependenciesOpen `setup_medgemma.ipynb` and follow the step-by-step instructions.

â”‚

â”œâ”€â”€ ğŸ“š docs/                   # Documentation## ğŸ“‹ Prerequisites

â”‚   â”œâ”€â”€ START_HERE.md              # â­ Quick start guide

â”‚   â”œâ”€â”€ QUICK_REFERENCE.md         # Quick lookup reference- **Hugging Face Account**: Create an account at [huggingface.co](https://huggingface.co)

â”‚   â”œâ”€â”€ HOW_TO_RUN_CELLS.md        # Detailed instructions- **Gemma Access**: Accept the license at [google/gemma-2b](https://huggingface.co/google/gemma-2b)

â”‚   â”œâ”€â”€ TROUBLESHOOTING.md         # Common issues & fixes- **HF Token**: Generate a token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

â”‚   â””â”€â”€ MEDGEMMA_4B_GUIDE.md       # MedGemma 4B info

â”‚## ğŸ”§ What's Included

â”œâ”€â”€ ğŸ“Š data/                   # Downloaded datasets

â”œâ”€â”€ ğŸ“ outputs/                # Generated outputs### Dependencies

â”œâ”€â”€ ğŸ .venv/                  # Python virtual environment- **PyTorch & TensorFlow**: Deep learning frameworks

â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies- **Transformers**: Hugging Face library for LLMs

â””â”€â”€ ğŸ“„ .gitignore             # Git ignore rules- **Jupyter**: Interactive notebook environment

```- **Medical NLP tools**: Specialized libraries for medical text processing

- **Visualization**: Matplotlib, Seaborn, Plotly

---

### Features

## âœ… What's Installed- 4-bit quantization for efficient memory usage

- Interactive chat interface

- **Python 3.9.13** in virtual environment- Batch processing capabilities

- **PyTorch 2.8.0+cpu** - Deep learning framework- Medical query examples

- **Transformers 4.57.1** - Hugging Face library- Model performance monitoring

- **BitsAndBytes 0.48.1** - 4-bit quantization

- **Jupyter Notebook** - Interactive environment## ğŸ’» Hardware Requirements

- **Data Science Stack** - NumPy, Pandas, Matplotlib, Seaborn

- **MedGemma 4B** - Medical-specific AI (4 billion parameters)### Minimum

- **RAM**: 8GB

---- **Storage**: 10GB free space

- **CPU**: Multi-core processor

## ğŸš€ Quick Start (3 Steps)

### Recommended

### **Step 1: Accept MedGemma License** (One-time!)- **GPU**: NVIDIA GPU with 8GB+ VRAM (for faster inference)

ğŸ‘‰ **https://huggingface.co/google/medgemma-4b-it**  - **RAM**: 16GB+

Click: **"Agree and access repository"**- **Storage**: 20GB+ free space



### **Step 2: Launch MedGemma**## ğŸ“š Model Variants

```powershell

.\scripts\launch_medgemma.ps1You can work with different Gemma models:

```- `google/gemma-2b` - 2 billion parameters (lighter)

- `google/gemma-7b` - 7 billion parameters (more capable)

### **Step 3: Run Cells**- `google/gemma-2b-it` - Instruction-tuned variant

1. Browser opens with notebook

2. **Top right** â†’ Select: `Python 3.9 (GEMAA)`## ğŸ”’ Security Note

3. Run cells: `Shift + Enter`

4. Ask medical questions! ğŸ¥**Never commit your Hugging Face token to version control!**



---Create a `.env` file (already in .gitignore) to store sensitive data:

```

## ğŸ““ NotebooksHF_TOKEN=your_token_here

```

| Notebook | Purpose |

|----------|---------|## ğŸ“– Usage Examples

| **`medgemma_actual.ipynb`** | Main MedGemma 4B AI (token configured âœ…) |

| **`real_world_datasets.ipynb`** | Analyze real medical data |### Basic Text Generation

| **`start_here.ipynb`** | Sample data analysis |```python

| **`test_cells.ipynb`** | Test environment |from transformers import AutoTokenizer, AutoModelForCausalLM



---tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")

model = AutoModelForCausalLM.from_pretrained("google/gemma-2b")

## ğŸ”§ Scripts

prompt = "What are the symptoms of diabetes?"

| Command | Purpose |inputs = tokenizer(prompt, return_tensors="pt")

|---------|---------|outputs = model.generate(**inputs, max_new_tokens=100)

| `.\scripts\launch_medgemma.ps1` | ğŸš€ Launch MedGemma notebook |response = tokenizer.decode(outputs[0])

| `.\scripts\launch_jupyter.ps1` | Launch Jupyter file browser |print(response)

| `.venv\Scripts\python.exe scripts\diagnose_jupyter.py` | Diagnose issues |```



---### Medical Question Answering

See `setup_medgemma.ipynb` for comprehensive examples of:

## ğŸ“š Documentation- Interactive chat

- Batch processing

All guides in `docs/` folder:- Fine-tuning preparation

- **`START_HERE.md`** - Complete setup guide â­- Performance monitoring

- **`QUICK_REFERENCE.md`** - Fast lookup

- **`HOW_TO_RUN_CELLS.md`** - Cell execution guide## ğŸ› ï¸ Troubleshooting

- **`TROUBLESHOOTING.md`** - Problem solutions

### Out of Memory Error

---- Use 4-bit or 8-bit quantization

- Try a smaller model (gemma-2b instead of gemma-7b)

## ğŸ¯ Common Tasks- Reduce batch size



### Ask Medical Questions### Model Download Issues

```powershell- Check your internet connection

.\scripts\launch_medgemma.ps1- Verify Hugging Face token is valid

```- Ensure you've accepted the Gemma license



### Analyze Real Medical Data### CUDA Not Available

```powershell- Install CUDA toolkit if you have an NVIDIA GPU

.\scripts\launch_jupyter.ps1- Update GPU drivers

# Then open: notebooks/real_world_datasets.ipynb- The model will fall back to CPU (slower but functional)

```

## ğŸ“ License

### Test Setup

```powershellThis project uses Google's Gemma models, which require accepting their license agreement.

.venv\Scripts\python.exe scripts\diagnose_jupyter.py

```## ğŸ¤ Contributing



---Feel free to open issues or submit pull requests to improve this setup!



## âš ï¸ Important## ğŸ“ Support



### Before First Use:For issues with:

1. âœ… Accept license: https://huggingface.co/google/medgemma-4b-it- Gemma models: Visit [Hugging Face Gemma page](https://huggingface.co/google/gemma-2b)

2. âœ… Select kernel: `Python 3.9 (GEMAA)`- This setup: Open an issue in this repository

3. â³ First run: Cell 3 downloads 2GB model (5-10 min)

---

### Token Configured:

- âœ… Already in `notebooks/medgemma_actual.ipynb`**Happy Medical AI Development! ğŸ¥ğŸ¤–**

- âœ… No manual login needed
- âš ï¸ Don't commit to public repos

---

## ğŸ†˜ Troubleshooting

**Cells don't run?**  
â†’ Select kernel: `Python 3.9 (GEMAA)` (top right)

**"No module named 'torch'"?**  
â†’ Wrong kernel selected

**Cell shows `[*]` forever?**  
â†’ If Cell 3: Normal (downloading model)

See `docs/TROUBLESHOOTING.md` for more help.

---

## ğŸ“ Disclaimer

**For educational purposes only.**  
Always consult healthcare professionals for medical advice.

---

## ğŸ‰ You're Ready!

1. Accept license: https://huggingface.co/google/medgemma-4b-it
2. Run: `.\scripts\launch_medgemma.ps1`
3. Select kernel: `Python 3.9 (GEMAA)`
4. Ask questions! ğŸ¥

**Happy Medical AI Exploration!** ğŸš€
